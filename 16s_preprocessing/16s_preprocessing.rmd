---
title: "Denoising of 16s rRNA data provided for cellulase project"
output: pdf_document
---
The file contains preprocessing steps for 16s rRNA amplicon data. It includes DADA2 pipeline and taxonomy annotation. Primer excision was done by cutadapt. DADA2 performed with pseudo-pooling. After inferring, merging (6723 sequences) and chimera removing we got 3127 sequences. Taxonomy prediction was done by Idtaxa with 80% confidence and gave us 94 sequences in the end of preprocessing. We left taxa identified at least at genus level.

```{r message=FALSE, warning=FALSE}
library(dada2)
library(phyloseq)
library(DECIPHER)
library(dplyr)
library(ggplot2)
library(reshape2)
library(ShortRead)
library(Biostrings)

set.seed(139)

```


Specify sample files and extract sample names

```{r}
root.raw <- '/home/is7/cellulase/amplicon/reads'
```

```{r}
forward.raw <- sort(list.files(root.raw, pattern="*R1_001.fastq.gz", full.names = TRUE))
reverse.raw <- sort(list.files(root.raw, pattern="*R2_001.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(forward.raw), "_"), `[`, 1)

```

```{r}
root.noprimer = '/home/is6/16s_preprocessing/filtN'

FWD <- "GTGCCAGCMGCCGCGGTAA"
REV <- "GGACTACVSGGGTATCTAAT"


#Removal of ambiguous bases
forward.filtN <- file.path(root.noprimer, basename(forward.raw)) # Put N-filterd files in filtN/ subdirectory
reverse.filtN <- file.path(root.noprimer, basename(reverse.raw))
filterAndTrim(forward.raw, forward.filtN, reverse.raw, reverse.filtN, maxN = 0, multithread = FALSE)
```

```{r}
cutadapt <- "/home/is6/.conda/envs/cellulase_project/bin/cutadapt"
```
```{r}

root.cut <- '/home/is6/16s_preprocessing/cutadapt'
if(!dir.exists(root.cut)) dir.create(root.cut)
forward.cut <- file.path(root.cut, basename(forward.raw))
reverse.cut <- file.path(root.cut, basename(reverse.raw))


FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC)
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)
# Run Cutadapt

for(i in seq_along(forward.raw)) {
system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2,  "-m",25,
"-o", forward.cut[i], "-p", reverse.cut[i], # output files
forward.filtN[i], reverse.filtN[i])) # input files
}

```


```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

```{r}

#As all files are creating using one library preparation method, I will just check one set of paired end FASTQ files!
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = forward.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = reverse.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = forward.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = reverse.cut[[1]]))
```

```{r}
#plotQualityProfile(forward.cut, aggregate=TRUE, n=100000)
#plotQualityProfile(forward.cut[1:2])
```

```{r}
# Open jpeg file
jpeg("qplot_FWD.jpg", width = 350, height = 350)
plotQualityProfile(forward.cut, aggregate=TRUE, n=100000)
dev.off()
```

Reverse reads
```{r}
jpeg("qplot_rev.jpg", width = 350, height = 350)
plotQualityProfile(reverse.cut, aggregate=TRUE, n=100000)
#plotQualityProfile(reverse.cut[1:2])
dev.off()
```

Filter and trim reads

QC after trimming

```{r}
root.qc = '/home/is6/16s_preprocessing/qc' 
 
forward.qc <- file.path(root.qc, basename(forward.cut))
reverse.qc <- file.path(root.qc, basename(reverse.cut))

qc.out <- filterAndTrim(forward.cut, forward.qc, reverse.cut, reverse.qc, truncLen=c(250, 200),
                        maxEE=c(2,2), compress=TRUE, multithread=TRUE)

qc.out

```


```{r}
jpeg("qplot_filt_FWD.jpg", width = 350, height = 350)
plotQualityProfile(forward.qc, aggregate=TRUE, n=100000)
#plotQualityProfile(reverse.cut[1:2])
dev.off()

```

```{r}
jpeg("qplot_filt_rev.jpg", width = 350, height = 350)
plotQualityProfile(reverse.qc, aggregate=TRUE, n=100000)
#plotQualityProfile(reverse.cut[1:2])
dev.off()

```

Fit error models

Forward reads
```{r}
#every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data

forward.err <- learnErrors(forward.qc, multithread=TRUE, MAX_CONSIST=20)
```

Reverse reads
```{r}
reverse.err <- learnErrors(reverse.qc, multithread=TRUE, MAX_CONSIST=20)
```


```{r}
jpeg("forward.err.jpg", width = 350, height = 350)
plotErrors(forward.err, nominalQ=TRUE)
#plotQualityProfile(reverse.cut[1:2])
dev.off()


#Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. It looks reasonable.
```
Dereplicate
```{r}
#combininges all identical sequencing reads into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence
forward.derep <- derepFastq(forward.qc, n=1e7, verbose=TRUE)
reverse.derep <- derepFastq(reverse.qc, n=1e7, verbose=TRUE)

# Name the derep-class objects by the sample names
names(forward.derep) <- sample.names
names(reverse.derep) <- sample.names
```

Infer sample composition

```{r}
#dereplicating
forward.dada <- dada(forward.derep, err=forward.err, pool='pseudo', multithread=TRUE)
reverse.dada <- dada(reverse.derep, err=reverse.err, pool='pseudo', multithread=TRUE)
```

Merge pairs

```{r}
merged <- mergePairs(forward.dada, forward.derep, reverse.dada, reverse.derep, verbose=TRUE)
```

Build a sequence table and remove chimera

```{r}
#constructing an amplicon sequence variant table (ASV) table. The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants.

sequence.table <- makeSequenceTable(merged)
dim(sequence.table)
```

```{r}

table(nchar(getSequences(sequence.table)))
hist(nchar(getSequences(sequence.table)), main="Distribution of sequence lengths")
```

```{r}
#Chimeric sequences are reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences!

sequence.table.nochim <- removeBimeraDenovo(sequence.table, method="consensus", multithread=TRUE, verbose=TRUE)
```

Summarise preprocessing statistics

```{r}
get.n <- function(x) sum(getUniques(x))
statistics <- cbind(qc.out, 
               sapply(forward.dada, get.n), 
               sapply(reverse.dada, get.n), 
               sapply(merged, get.n),
               rowSums(sequence.table.nochim))
colnames(statistics) <- c('noprimer_pairs', 'qc_pairs', 'forward_denoised', 'reverse_denoised', 'merged', 'non-chimeric')
rownames(statistics) <- sample.names
head(statistics)
```
```{r}
write.table(statistics, file = "/home/is6/16s_preprocessing/statistics.tsv", row.names=FALSE, sep="\t")
```

Extract sequences and sequence counts, subset sequences observed at least 3 times in at least 20% of the samples.

```{r}
# extract results
seqs <- getSequences(sequence.table.nochim)
seq.names <- paste0("seq", seq(length(seqs)))
names(seqs) <- seq.names

count.table <- otu_table(sequence.table.nochim, taxa_are_rows=FALSE) %>% t
rownames(count.table) <- seq.names

# subset results
freq.mask <- (count.table >= 3) %>%
    rowSums %>%
    `/`(ncol(count.table)) %>%
    `>=`(0.2)

seqs.subset <- DNAStringSet(seqs[freq.mask], use.names=TRUE)
count.table.subset <- count.table[freq.mask]

cat('Spared', length(seqs.subset), 'out of', length(seqs),
    'sequences\n')
cat('Spared', sum(count.table.subset), 'out of', sum(count.table),
    'observations\n')
```

Write results

```{r}
output_root = '/home/is6/16s_preprocessing/dada2'
dir.create(output_root, showWarnings=FALSE)

write.table(statistics, '/home/is6/16s_preprocessing/dada2/stats_16s.tsv', sep='\t', quote=FALSE, col.names=NA)

writeXStringSet(seqs.set, '/home/is6/16s_preprocessing/dada2/sequences_16s.fna')
write.table(count.table.subset, '/home/is6/16s_preprocessing/dada2/counts_16s.tsv', sep='\t', quote=FALSE, col.names=NA)
```

```{r}
dir.create('/home/is6/16s_preprocessing/plots', showWarnings = FALSE)

colnames(statistics) <- c('Primer excisition', 'Quality control', 'Denoising (R1)', 'Denoising (R2)', 'Pair merging', 'Chimera elimination')
statistics.long <- melt(statistics)

(plot.stats <- ggplot(statistics.long, aes(x=Var2, y=value / 10^3)) + 
    geom_boxplot() +
    scale_x_discrete(limits = rev(levels(statistics.long$variable))) +
    ylab(bquote('Sequence count, '*10^3*'')) +
    coord_flip() +
    theme_bw() +
    theme(text=element_text(size=11,  family='Helvetica'),
          axis.title.y=element_blank())
)

ggsave(file='/home/is6/16s_preprocessing/plots/stats_is6.svg', plot=plot.stats, width=6, height=2)

```
Predict taxonomy

```{r}
classifier <- readRDS('/home/ilia/projects/arriam/idtaxa/classifier_v4.RData')
predicted.taxonomy <- IdTaxa(seqs.subset, classifier, threshold=80, processors=30)
format.taxonomy <- function(id) paste(id$taxon, sep='', collapse=';')
tax.ids <- sapply(predicted.taxonomy, format.taxonomy)


write.table(as.data.frame(tax.ids, row.names=names(tax.ids)), '/home/is6/16s_preprocessing/dada2/taxonomy.tsv', sep='\t', quote=FALSE, col.names=FALSE)

```
Cleaning taxonomy table

Leaving sequences with identified ranks at genus level

```{r}
# separate table into column with corresponded ranks
taxonomy <- as.data.frame(tax.ids, row.names=names(tax.ids)) %>% 
  separate(., col = tax.ids, into = c('root', 'domain', 'phylum', 'class', 'order', 'family', 'genus', 'species'), sep = ';')

# leave only that rows which have information up to genus level
taxonomy <- taxonomy[complete.cases(taxonomy$genus), ]

write.table(taxonomy, '/home/is6/16s_preprocessing/dada2/taxonomy_clean.tsv', sep='\t', quote=FALSE, col.names=FALSE)
```


